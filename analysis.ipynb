{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d84bdc-8072-4fc1-8033-dc9ef2276951",
   "metadata": {},
   "source": [
    "# Week 1 Group Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70906237-8768-40f0-8d7f-653d106b2462",
   "metadata": {},
   "source": [
    "This notebook is a template workspace for the week 1 group projects involving\n",
    "the [Reproducible Brain Charts](https://reprobrainchart.github.io/) (RBC)\n",
    "database. Herein, we demonstrate how to access the RBC data and demonstrate a\n",
    "linear regression analysis on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953b424-fa57-4c10-b000-347a42254629",
   "metadata": {},
   "source": [
    "## Getting Started with RBC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5fa5-4ed4-44f8-b07c-e6252d450cf1",
   "metadata": {},
   "source": [
    "To load in some of the RBC data, we'll use some tools already installed on the\n",
    "HUB: `rbclib` and `pandas`.\n",
    "The `rbclib` library allows us to access RBC data from the cloud; it is\n",
    "demonstrated below.\n",
    "\n",
    "The `pandas` library handles spreadsheet data (called `DataFrame`s in Python)\n",
    "and can read tab-separated and comma-separated value files (`*.tsv` and\n",
    "`*.csv` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9811e8-12e7-4133-89e5-bce8eef6b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need the RBCPath type from the rbclib package to load data from the RBC.\n",
    "from rbclib import RBCPath\n",
    "\n",
    "# We'll also want to load some data directly from the filesystem.\n",
    "from pathlib import Path\n",
    "\n",
    "# We'll want to load/process some of the data using pandas and numpy.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e96e6-8fa1-4b3d-a952-bc2bb3ef26e7",
   "metadata": {},
   "source": [
    "### Accessing the PNC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bed2a-3869-47c3-9491-cfa57acb0688",
   "metadata": {},
   "source": [
    "The RBC project contains many datasets; for this project, we will focus on\n",
    "just one of these datasets: the\n",
    "[Philadelphia Neurodevelopmental Cohort](\n",
    "https://www.med.upenn.edu/bbl/philadelphianeurodevelopmentalcohort.html)\n",
    "(PNC). The PNC contains a lot of data, including raw MRI data. However, due to\n",
    "the time constraints for this project, we suggest that teams focus on the\n",
    "already processed data provided by the RBC, which is described below.\n",
    "\n",
    "The RBC's data is stored in a combination of GitHub repositories and Amazon S3\n",
    "buckets. The RBC GitHub repositories all belong to the organization\n",
    "[`ReproBrainChart`](https://github.com/ReproBrainChart), and each contains a\n",
    "subset of the data for one of the RBC datasets; for the PNC dataset, all\n",
    "repositories names start with `PNC_`:\n",
    "\n",
    "* `PNC_FreeSurfer`: structural data processed by FreeSurfer.\n",
    "* `PNC_BIDS`: raw MRI scan data in the\n",
    "  [Brain Imaging Data Structure](https://bids.neuroimaging.io/index.html)\n",
    "  format.\n",
    "* `PNC_CPAC`: processed functional MRI data.\n",
    "\n",
    "One typically accesses the RBC using the [`datalad`](https://www.datalad.org/)\n",
    "tool (see the [RBC page on accessing the data](\n",
    "https://reprobrainchart.github.io/docs/get_data) for more information).\n",
    "However, we will access the data using the `RBCPath` type that was imported in\n",
    "the code-cell above (`from rbclib import RBCPath`). This type inherits from a\n",
    "type called `CloudPath` (from the library [`cloudpathlib`](\n",
    "https://cloudpathlib.drivendata.org/stable/)); it represents the path of a\n",
    "file in the RBC dataset and can be used to access data in thecloud as if it\n",
    "were local.\n",
    "\n",
    "For example, the following cell creates an `RBCPath` to a subject's FreeSurfer\n",
    "data directory then lists and prints the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3489ab5-effd-408e-a633-88ddd68be768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path refers to the repo github.com:ReproBrainChart/PNC_FreeSurfer;\n",
    "# Subject 1000393599's directory is used as an example.\n",
    "subject_id = 1000393599\n",
    "# To browse the repo, use this link:\n",
    "# https://github.com/ReproBrainChart/PNC_FreeSurfer/tree/main\n",
    "sub_path = RBCPath(f'rbc://PNC_FreeSurfer/freesurfer/sub-{subject_id}')\n",
    "\n",
    "# This path refers to a directory:\n",
    "assert sub_path.is_dir()\n",
    "\n",
    "# Print each file in the directory:\n",
    "for file in sub_path.iterdir():\n",
    "    print(repr(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726ff69-66cc-4ec7-84f4-04421055d7a6",
   "metadata": {},
   "source": [
    "If we want to open and load one of these files, we can do so using the\n",
    "`RBCPath.open` method. This method is like the `Path.open` method (from the\n",
    "built-in Python library [`pathlib`](1)). For example, if we want to load this\n",
    "subject's `regionsurfacestats.tsv` file, we can do so as follows.\n",
    "\n",
    "[1]: https://docs.python.org/3/library/pathlib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a190573-7d9d-4260-9986-0290e9b5adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can construct new paths by using the `/` operator. This is identical to\n",
    "# how paths are constructed in the `pathlib` module.\n",
    "stats_filepath = sub_path / f'sub-{subject_id}_regionsurfacestats.tsv'\n",
    "\n",
    "# Use pandas to read in the TSV file then display it:\n",
    "\n",
    "print(f\"Loading {stats_filepath} ...\")\n",
    "with stats_filepath.open('r') as f:\n",
    "    data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac0df6-9208-4d85-8a70-044f60ae2f6c",
   "metadata": {},
   "source": [
    "### Getting the Participant Lists and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b349df-15c8-42a0-a18b-1a7e6a6b28c2",
   "metadata": {},
   "source": [
    "We have pre-sorted the participants in the PNC study into a training and a\n",
    "test dataset. Basic metadata about each participant can be found in TSV files\n",
    "in the `shared` directory in your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1032c95-1ca5-4ef5-96d1-8f51acf260d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant meta-data is generally located in the BIDS repository for each\n",
    "# study:\n",
    "rbcdata_path = Path('/home/jovyan/shared/data/RBC')\n",
    "train_filepath = rbcdata_path / 'train_participants.tsv'\n",
    "test_filepath = rbcdata_path / 'test_participants.tsv'\n",
    "\n",
    "# Load the PNC participants TSV files...\n",
    "with train_filepath.open('r') as f:\n",
    "    train_data = pd.read_csv(f, sep='\\t')\n",
    "with test_filepath.open('r') as f:\n",
    "    test_data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# We can also concatenate the two datasets into a single dataset of all\n",
    "# study participants:\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "\n",
    "# Display the full dataframe:\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd455303-7695-4aff-b2fa-b4b3435f8e7c",
   "metadata": {},
   "source": [
    "## Project Goal: Predict the `p_factor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbcabd-b561-4126-a21d-e535ae259ae6",
   "metadata": {},
   "source": [
    "The RBC datasets include a variable for each subject called the `p_factor`.\n",
    "This factor is intended to capture overall psychopathology and is discussed at\n",
    "length in RBC publications. The goal for this project is to train a\n",
    "machine-learning tool to predict the `p_factor` of each participant in the\n",
    "test dataset by using data from the participants in the training dataset.\n",
    "Note that the `p_factor` column in the training dataset is provided, but the\n",
    "`p_factor` column in the test dataset has been set to `NaN`.\n",
    "\n",
    "Your specific task is to calculate predicted `p_factor` values, to insert\n",
    "these values into the `'p_factor'` column of the provided `test_data`\n",
    "dataframe, to save `test_data` to disk using the `test_data.to_csv` method\n",
    "(example below), then finally to commit and push the file to your group's\n",
    "GitHub repository.\n",
    "\n",
    "We will look over the results of the group mini-projects together once\n",
    "everyone has submitted their predictions.\n",
    "\n",
    "**In this section, we demonstrate an example approach to predicting the\n",
    "`p_factor` using one of the most straightforward supervised techniques in\n",
    "machine learning: linear regression.** Suppose we suspected that the size of\n",
    "Brodmann Area 1 was predictive of the `p_factor` in individual participants\n",
    "and thus wanted to run a linear regression analysis to predict `p_factor` in\n",
    "the test participants based on the relationship in the training participants.\n",
    "Performing linear regression will require a few steps, which are likely to be\n",
    "similar in your projects:\n",
    "\n",
    "1. Collect the relevant data (the surface areas of BA1) into a dataframe.\n",
    "2. Train the linear regression model using the training participants.\n",
    "3. Use the trained model to predict the `p_factor` of the test subjects.\n",
    "4. Export and commit our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929ed1a-02a9-424a-b05c-6f97e71bf052",
   "metadata": {},
   "source": [
    "### Step 1. Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a5bad-e052-44c2-8064-ebf76a121284",
   "metadata": {},
   "source": [
    "The data we need to make the predictions are, for each participant, (1) the\n",
    "surface area of BA1, and (2) the `p_factor`. We can collect these into a\n",
    "dataframe using `pandas` and the `RBCPath` type (to load the data).\n",
    "\n",
    "The surface area of BA1 can be found in the FreeSurfer TSV files examined\n",
    "earlier in this notebook. We'll start by writing a function that loads the\n",
    "appropriate TSV for for a given participant.\n",
    "\n",
    "In order to speed up the loading of data during the project, we can specify\n",
    "a `local_cache_dir` where any data downloaded using the function will be\n",
    "automatically saved; the next time you load the same data, it will be loaded\n",
    "from local storage instead of from S3 (local storage is much faster). The\n",
    "function here uses the directory `cache` in your home directory by default,\n",
    "but you can change this if you prefer a different directory. The directory\n",
    "will be automatically created for you if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74b7d4-132f-4ce8-bc02-2d3a6fa1b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fsdata(participant_id, local_cache_dir=(Path.home() / 'cache')):\n",
    "    \"Loads and returns the dataframe of a PNC participant's FreeSurfer data.\"\n",
    "\n",
    "    # Check that the local_cache_dir exists and make it if it doesn't.\n",
    "    if local_cache_dir is not None:\n",
    "        local_cache_dir = Path(local_cache_dir)\n",
    "        local_cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Make the RBCPath and find the appropriate file:\n",
    "    pnc_freesurfer_path = RBCPath(\n",
    "        'rbc://PNC_FreeSurfer/freesurfer',\n",
    "        # We provide the local_cache_dir to the RBCPath object; all paths made\n",
    "        # from this object will use the same cache directory.\n",
    "        local_cache_dir=local_cache_dir)\n",
    "    participant_path = pnc_freesurfer_path / f'sub-{participant_id}'\n",
    "    tsv_path = participant_path / f'sub-{participant_id}_regionsurfacestats.tsv'\n",
    "\n",
    "    # Use pandas to read in the TSV file:\n",
    "    with tsv_path.open('r') as f:\n",
    "        data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "    # Return the loaded data:\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7ecff-4fa0-47b8-9be4-b133d2f8c4e0",
   "metadata": {},
   "source": [
    "We can run this function to obtain a subject's FreeSurfer dataframe. This\n",
    "dataframe contains information about various anatomical atlases that segment\n",
    "the cortical surface into distinct regions. Notice that the `atlas` column of\n",
    "the dataframe contains the name of distinct atlases while the `StructName`\n",
    "column contains the name of the ROI described. The `SurfArea` column gives the\n",
    "surface area of each ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a479fe4-e051-442d-a780-c036d3ff91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_participant_id = 1000393599\n",
    "data = load_fsdata(example_participant_id)\n",
    "\n",
    "# Display the dataframe we loaded:\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93998d-0b3a-4aad-ba48-8640e7634f5c",
   "metadata": {},
   "source": [
    "To extract the surface area of BA1, we need to look for rows whose\n",
    "`StructName` indicates that it represents BA1. In the RBC database, the name\n",
    "`'Brodmann.1'` is used to represent Brodmann Area 1. We can select only the\n",
    "rows of interest using this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71c78d-1d54-420c-b908-a0f352c4e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_mask = (data['StructName'] == 'Brodmann.1')\n",
    "data[row_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c02b4a-7515-404a-86a2-c7c2286e727e",
   "metadata": {},
   "source": [
    "Given these rows, we can extract the BA1 surface areas and sum them (we will\n",
    "perform the linear regression on the bilateral BA1 surface area by adding the\n",
    "left and right hemisphere surface areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be77dd-3a54-4f46-8d08-ced65589622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba1_surfareas = data.loc[row_mask, 'SurfArea']\n",
    "ba1_surfarea = sum(ba1_surfareas)\n",
    "\n",
    "# Show the bilateral surface area for this participant (in square mm):\n",
    "ba1_surfarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af5c9a-19e5-499d-9ced-792a6e0010da",
   "metadata": {},
   "source": [
    "Based on the above workflow, we can now write a function that extracts the BA1\n",
    "surface area for a participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27acf24-98b7-4388-b5a5-13ecee644ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ba1_surfarea(participant_id):\n",
    "    \"\"\"Loads and returns the bilateral Brodmann Area 1 surface area for a PNC\n",
    "    study participant.\n",
    "    \"\"\"\n",
    "    # First, load the subject's FreeSurfer dataframe:\n",
    "    data = load_fsdata(participant_id)\n",
    "    # Next, find the relevant rows:\n",
    "    row_mask = (data['StructName'] == 'Brodmann.1')\n",
    "    # Then extract and sum the surface areas:\n",
    "    ba1_surfareas = data.loc[row_mask, 'SurfArea']\n",
    "    ba1_surfarea = sum(ba1_surfareas)\n",
    "    # And return this value:\n",
    "    return ba1_surfarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae057b10-19ad-46ec-b276-0d2ea4d1c766",
   "metadata": {},
   "source": [
    "Let's test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63231c34-aad6-4c89-ba69-e79d60ed2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ba1_surfarea(example_participant_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadf9f4-d35b-4d5b-ab51-34819905f396",
   "metadata": {},
   "source": [
    "Now that we have code written to extract data from a single subject, we can\n",
    "gather the BA1 data for our training and test subjects into a single\n",
    "dataframe. Doing so will require downloading all of the TSV files for all of\n",
    "the subjects in the training dataset. This will take some time, but probably\n",
    "less than an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d268e-d0e8-48b4-b1ba-3006ba536d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load in surface area data for each participant:\n",
    "print(\"Loading surface areas...\")     \n",
    "\n",
    "# We will put the rows in this dictionary of lists as we build the dataframe:\n",
    "all_vars = {\n",
    "    'participant_id': [],\n",
    "    'ba1_surface_area': [],\n",
    "    'p_factor': []}\n",
    "\n",
    "# We'll display a progress bar `prog` as we go also:\n",
    "from ipywidgets import IntProgress\n",
    "prog = IntProgress(min=0, max=len(all_data))\n",
    "display(prog)\n",
    "\n",
    "# Okay, loop through each row of the `all_data` dataframe, which contains both\n",
    "# training and test subjects, load their BA1 data, and store it in the\n",
    "# all_vars dictionary.\n",
    "for (ii, row) in all_data.iterrows():\n",
    "    # Extract the participant ID and p_factor (which will be NaN for test\n",
    "    # participants).\n",
    "    participant_id = row['participant_id']\n",
    "    p_factor = row['p_factor']\n",
    "    # Load the surface area for this participant:\n",
    "    try:\n",
    "        surf_area = load_ba1_surfarea(participant_id)\n",
    "    except FileNotFoundError:\n",
    "        # Some subjects are just missing the file, so we code them as NaN.\n",
    "        surf_area = np.nan\n",
    "    # Append the participant ID and their surface area to our dataset:\n",
    "    all_vars['participant_id'].append(participant_id)\n",
    "    all_vars['ba1_surface_area'].append(surf_area)\n",
    "    all_vars['p_factor'].append(p_factor)\n",
    "    # Increment the progress bar counter:\n",
    "    prog.value += 1\n",
    "\n",
    "# Convert train_vars into a dataframe.\n",
    "all_vars = pd.DataFrame(all_vars)\n",
    "\n",
    "# Extract the training and test subjects into separate dataframes; the test\n",
    "# participants can be identified as those having NaN values for their\n",
    "# p_factor column.\n",
    "train_vars = all_vars[~np.isnan(all_vars['p_factor'])]\n",
    "test_vars = all_vars[np.isnan(all_vars['p_factor'])]\n",
    "\n",
    "# Display the finished dataframe.\n",
    "all_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80cd37-6807-45cb-81ae-6f0369a0540c",
   "metadata": {},
   "source": [
    "### Step 2. Train the Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff330c49-67db-45bc-afc2-63ff895424a4",
   "metadata": {},
   "source": [
    "To train and perform the linear regression analysis, we will use the\n",
    "[`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "type from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a65c2-03ae-44f8-8ddf-58989aa9c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression type:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# LinearRegression requires a matrix whose columns are the variables and whose\n",
    "# final column is the value being predicted (the p_factor for us). We can\n",
    "# extract these columns straight from the dataframes we generated.\n",
    "train_matrix = train_vars.loc[:, ['ba1_surface_area', 'p_factor']].values\n",
    "# We need to exclude rows with NaNs for training:\n",
    "train_okrows = np.all(~np.isnan(train_matrix), axis=1)\n",
    "train_matrix = train_matrix[train_okrows]\n",
    "\n",
    "# Train the regression using the training matrix:\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(train_matrix[:, :1], train_matrix[:, 1])\n",
    "\n",
    "# Display the trained regression parameters:\n",
    "print(\"Linear Regression:\")\n",
    "print(\"  Intercept:\", lreg.intercept_)\n",
    "print(\"  Slope:\", lreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766245cf-94c9-42b0-a42b-2ed7190b2e80",
   "metadata": {},
   "source": [
    "### Step 3. Predict the `p_factor` of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06a34b-c206-4c8f-bc3a-fe7afee79edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply the trained linear regression object `lreg` to the 1-column\n",
    "# matrix of ba1_surface_area values in the test_vars dataframe.\n",
    "test_matrix = test_vars.loc[:, ['ba1_surface_area']].values\n",
    "test_okrows = np.all(~np.isnan(test_matrix), axis=1)\n",
    "test_matrix = test_matrix[test_okrows]\n",
    "\n",
    "# Apply the model:\n",
    "p_factor_predictions = lreg.predict(test_matrix)\n",
    "\n",
    "# Display the predictions:\n",
    "p_factor_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9dc36-0778-4fd5-9a92-598922c998b4",
   "metadata": {},
   "source": [
    "### Step 4. Save and Commit the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23498-57b9-4e6d-a103-4e1f4afdb177",
   "metadata": {},
   "source": [
    "To save and commit the results, we first need to save the predicted `p_factor`\n",
    "data into the test dataframe (where there are currently NaNs). In the cell\n",
    "above, we calculated the variable `test_okrows` that indicates which rows\n",
    "of the `test_vars`, `test_matrix`, and `test_data` objects were predicted\n",
    "(those that weren't predicted were excluded due to missing surface area data\n",
    "in our case).\n",
    "\n",
    "We can use this to insert the predicted `p_factor` data into `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d718f88-14b8-4b90-bf73-25e53dafc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[test_okrows, 'p_factor'] = p_factor_predictions\n",
    "\n",
    "# Display the resulting test data:\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d197ec-61ca-4225-ab51-82087d4b48e8",
   "metadata": {},
   "source": [
    "We now need to save the data to disk. We want to put this in the `results`\n",
    "directory of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5bcfe-3ff9-4c78-9782-1777a1ff9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sep='\\t' option here is necessary for tab-separated-value (as opposed to\n",
    "# comma-separated-value) files. The `index=False` just indicates that pandas\n",
    "# doesn't need to write out its own index column.\n",
    "\n",
    "group_name = 'example'  # Change this to be your group name!\n",
    "\n",
    "test_data.to_csv(f'results/{group_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4f18b-72b8-4b46-9ff6-bcdf1b699d35",
   "metadata": {},
   "source": [
    "Once the tsv file has been saved, you can commit it to your GitHub repository\n",
    "then push it and submit a pull request to the `results` branch of the original\n",
    "repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
